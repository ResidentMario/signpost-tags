{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "with open(\"../data/signpost_corpus.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    sentences = nltk.sent_tokenize(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This is, incidentally, a pretty good example of a case when piping (via something akin to pandas.pipe) would be useful.\n",
    "# This expression is effectively unreadable, and no easy alternatives exist.\n",
    "sentences_pos = np.concatenate(\n",
    "    [nltk.pos_tag(\n",
    "            [word for word in nltk.word_tokenize(sentence) if len(word) <= 34]  # \"Supercalifragilisticexpialidocious\"\n",
    "        ) for sentence in sentences\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U34')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pos.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory usage is barely anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_d = nltk.FreqDist(sentences_pos[...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common elements are the usual suspects (mostly stopwords). Note also that I haven't applied any stemming yet, so `running` and `run` will be reported seperately for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 211342),\n",
       " ('the', 204079),\n",
       " ('.', 161247),\n",
       " ('of', 122910),\n",
       " ('to', 98783),\n",
       " ('and', 97656),\n",
       " ('a', 79379),\n",
       " ('in', 65785),\n",
       " (')', 49361),\n",
       " ('(', 49048)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_d.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since right now we're playing with parts of speech..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 608156),\n",
       " ('IN', 509969),\n",
       " ('NNP', 487714),\n",
       " ('DT', 414182),\n",
       " ('JJ', 307300),\n",
       " ('NNS', 265547),\n",
       " (',', 211342),\n",
       " ('.', 172903),\n",
       " ('RB', 150107),\n",
       " ('VBN', 131700)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(sentences_pos[...,1]).most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up what these are using the help system, but...eh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33342"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_d['Wikipedia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_pos = sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('article', 13894),\n",
       " ('week', 9839),\n",
       " ('nom', 8025),\n",
       " ('project', 7706),\n",
       " ('page', 6301),\n",
       " ('case', 5727),\n",
       " ('time', 5640),\n",
       " ('community', 4483),\n",
       " ('year', 4042),\n",
       " ('number', 3657),\n",
       " ('list', 3596),\n",
       " ('information', 3511),\n",
       " ('content', 3410),\n",
       " ('work', 3302),\n",
       " ('%', 3080),\n",
       " ('discussion', 2843),\n",
       " ('process', 2747),\n",
       " ('status', 2738),\n",
       " ('coverage', 2627),\n",
       " ('editor', 2364),\n",
       " ('news', 2314),\n",
       " ('way', 2298),\n",
       " ('part', 2233),\n",
       " (']', 2190),\n",
       " ('talk', 2179)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"NN\"],0]\n",
    ").most_common()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 122910), ('in', 65785), ('on', 41511), ('for', 38708), ('by', 30754)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"IN\"],0]\n",
    ").most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 33192),\n",
       " ('Wikimedia', 9857),\n",
       " ('Foundation', 5629),\n",
       " ('Signpost', 3931),\n",
       " ('WikiProject', 3733),\n",
       " ('New', 2837),\n",
       " ('WMF', 2740),\n",
       " ('English', 2472),\n",
       " ('Committee', 2365),\n",
       " ('Wales', 2342),\n",
       " ('Board', 1942),\n",
       " ('Featured', 1910),\n",
       " ('United', 1702),\n",
       " ('User', 1647),\n",
       " ('January', 1634),\n",
       " ('April', 1600),\n",
       " ('May', 1563),\n",
       " ('List', 1545),\n",
       " ('US', 1525),\n",
       " ('March', 1521),\n",
       " ('August', 1508),\n",
       " (']', 1505),\n",
       " ('October', 1475),\n",
       " ('December', 1473),\n",
       " ('February', 1472)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"NNP\"],0]\n",
    ").most_common()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 204079), ('a', 79379), ('The', 35638), ('this', 18258), ('an', 17413)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"DT\"],0]\n",
    ").most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 8196),\n",
       " ('other', 8059),\n",
       " ('last', 4818),\n",
       " ('many', 4361),\n",
       " ('such', 4137),\n",
       " ('first', 4067),\n",
       " ('featured', 3922),\n",
       " ('good', 2762),\n",
       " ('several', 2553),\n",
       " ('few', 2221),\n",
       " ('same', 1868),\n",
       " ('active', 1820),\n",
       " ('recent', 1818),\n",
       " ('nom', 1801),\n",
       " ('own', 1771),\n",
       " ('different', 1765),\n",
       " ('related', 1745),\n",
       " ('current', 1716),\n",
       " ('open', 1714),\n",
       " ('available', 1634),\n",
       " ('English', 1612),\n",
       " ('important', 1597),\n",
       " ('much', 1544),\n",
       " ('German', 1493),\n",
       " ('public', 1487)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"JJ\"],0]\n",
    ").most_common()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('articles', 16781),\n",
       " ('editors', 6684),\n",
       " ('people', 4487),\n",
       " ('users', 4154),\n",
       " ('projects', 3662),\n",
       " ('pages', 3429),\n",
       " ('years', 2735),\n",
       " ('members', 2525),\n",
       " ('edits', 2480),\n",
       " ('changes', 2181),\n",
       " ('images', 2146),\n",
       " ('media', 2109),\n",
       " ('cases', 1949),\n",
       " ('topics', 1938),\n",
       " ('issues', 1921),\n",
       " ('lists', 1758),\n",
       " ('sources', 1749),\n",
       " ('others', 1644),\n",
       " ('pictures', 1450),\n",
       " ('candidates', 1421),\n",
       " ('results', 1375),\n",
       " ('things', 1364),\n",
       " ('months', 1353),\n",
       " ('links', 1232),\n",
       " ('students', 1228)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"NNS\"],0]\n",
    ").most_common()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 13761), ('also', 7118), (\"n't\", 5008), ('now', 4020), ('only', 3652)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"RB\"],0]\n",
    ").most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('been', 11312),\n",
       " ('nominated', 3784),\n",
       " ('created', 3580),\n",
       " ('promoted', 2543),\n",
       " ('used', 2215)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(\n",
    "    words_pos[[i for i, word_pos in enumerate(words_pos[...,1]) if word_pos == \"VBN\"],0]\n",
    ").most_common()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. You can already see how much value we would get out of further orthonormalization by stemming the data.\n",
    "\n",
    "A conditional frequency distribution can also be calculated, which reverses this: instead of getting a total from `freq['WORD']` we get a list of kinds of grammatical places where that word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VB', 48), ('VBN', 43), ('NN', 24), ('VBD', 21)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ConditionalFreqDist(words_pos.tolist())['cut'].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NNP', 2342), ('NNS', 157), ('VBZ', 5)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ConditionalFreqDist(words_pos.tolist())['Wales'].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.ConditionalFreqDist(wsj) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of using part-of-speech tagging to do something interesting via trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['The', 'DT'], \n",
       "       dtype='<U34'), array(['Association', 'NNP'], \n",
       "       dtype='<U34'), array(['of', 'IN'], \n",
       "       dtype='<U34'))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.trigrams(words_pos).__next__()  # it's a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verb_to_verb_trigrams = []\n",
    "for ((w1, pos1), (w2, pos2), (w3, pos3)) in nltk.trigrams(words_pos):\n",
    "    if pos1[0] == \"V\" and w2 == \"to\" and pos3[0] == \"V\":\n",
    "        verb_to_verb_trigrams.append(((w1, pos1), (w2, pos2), (w3, pos3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('have', 'VB'), ('to', 'TO'), ('wait', 'VB')),\n",
       " (('continued', 'VBD'), ('to', 'TO'), ('make', 'VB')),\n",
       " (('take', 'VB'), ('to', 'TO'), ('issue', 'VB')),\n",
       " (('managed', 'VBN'), ('to', 'TO'), ('begin', 'VB')),\n",
       " (('seemed', 'VBD'), ('to', 'TO'), ('be', 'VB')),\n",
       " (('proceeded', 'VBD'), ('to', 'TO'), ('add', 'VB')),\n",
       " (('had', 'VBD'), ('to', 'TO'), ('be', 'VB')),\n",
       " (('remains', 'VBZ'), ('to', 'TO'), ('be', 'VB')),\n",
       " (('voted', 'VBN'), ('to', 'TO'), ('accept', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('reject', 'VB')),\n",
       " (('decide', 'VB'), ('to', 'TO'), ('accept', 'VB')),\n",
       " (('begins', 'VBZ'), ('to', 'TO'), ('shift', 'VB')),\n",
       " (('fail', 'VBP'), ('to', 'TO'), ('recognize', 'VB')),\n",
       " (('appears', 'VBZ'), ('to', 'TO'), ('be', 'VB')),\n",
       " (('have', 'VB'), ('to', 'TO'), ('wait', 'VB')),\n",
       " (('doomed', 'VBN'), ('to', 'TO'), ('fail', 'VB')),\n",
       " (('failed', 'VBD'), ('to', 'TO'), ('reach', 'VB')),\n",
       " (('need', 'VBP'), ('to', 'TO'), ('be', 'VB')),\n",
       " (('chosen', 'VBN'), ('to', 'TO'), ('be', 'VB')),\n",
       " (('happen', 'VB'), ('to', 'TO'), ('read', 'VB'))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_to_verb_trigrams[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('voted', 'VBN'), ('to', 'TO'), ('accept', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('reject', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('hear', 'VB')),\n",
       " (('voted', 'VBN'), ('to', 'TO'), ('do', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('stay', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('accept', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('keep', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('reject', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('accept', 'VB')),\n",
       " (('voted', 'VBD'), ('to', 'TO'), ('close', 'VB'))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tri for tri in verb_to_verb_trigrams if tri[0][0] == 'voted'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accept', 12),\n",
       " ('reject', 6),\n",
       " ('approve', 4),\n",
       " ('advise', 3),\n",
       " ('keep', 3),\n",
       " ('admonish', 2),\n",
       " ('close', 2),\n",
       " ('stay', 2),\n",
       " ('affirm', 2),\n",
       " ('appoint', 1),\n",
       " ('purchase', 1),\n",
       " ('disband', 1),\n",
       " ('award', 1),\n",
       " ('hear', 1),\n",
       " ('change', 1),\n",
       " ('strip', 1),\n",
       " ('leave', 1),\n",
       " ('require', 1),\n",
       " ('adopt', 1),\n",
       " ('select', 1),\n",
       " ('modify', 1),\n",
       " ('open', 1),\n",
       " ('desysop', 1),\n",
       " ('reprimand', 1),\n",
       " ('do', 1),\n",
       " ('restore', 1),\n",
       " ('request', 1),\n",
       " ('have', 1),\n",
       " (']', 1),\n",
       " ('delete', 1),\n",
       " ('ban', 1),\n",
       " ('abolish', 1),\n",
       " ('suspend', 1),\n",
       " ('remove', 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist([tri[2][0] for tri in verb_to_verb_trigrams if tri[0][0] == 'voted']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del verb_to_verb_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jimbos = [trigram for trigram in nltk.trigrams(words_pos) if trigram[2][0] == \"Jimbo\"][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['.', '.'], [\"''\", \"''\"], ['Jimbo', 'NNP']],\n",
       " [['debate', 'NN'], [',', ','], ['Jimbo', 'NNP']],\n",
       " [['.', '.'], [\"''\", \"''\"], ['Jimbo', 'NNP']],\n",
       " [['rumor', 'NN'], ['that', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['approval', 'NN'], ['of', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['passed', 'VBD'], ['to', 'TO'], ['Jimbo', 'NNP']],\n",
       " [['email', 'NN'], ['to', 'TO'], ['Jimbo', 'NNP']],\n",
       " [['based', 'VBN'], ['on', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['banned', 'VBN'], ['by', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['Still', 'RB'], [',', ','], ['Jimbo', 'NNP']],\n",
       " [['statements', 'NNS'], ['by', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['involved', 'VBN'], ['.', '.'], ['Jimbo', 'NNP']],\n",
       " [['God-King', 'JJ'], [\"''\", \"''\"], ['Jimbo', 'NN']],\n",
       " [['lost', 'VBN'], [',', ','], ['Jimbo', 'NNP']],\n",
       " [['treatment', 'NN'], ['of', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['Grunt', 'NNP'], [',', ','], ['Jimbo', 'NNP']],\n",
       " [['relieved', 'JJ'], ['that', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['message', 'NN'], ['for', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['meetup', 'NN'], ['without', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['meetup', 'NN'], [',', ','], ['Jimbo', 'NNP']],\n",
       " [['appealed', 'VBD'], ['for', 'IN'], ['Jimbo', 'NNP']],\n",
       " [['initially', 'RB'], [',', ','], ['Jimbo', 'NNP']],\n",
       " [['Interestingly', 'NNP'], [',', ','], ['Jimbo', 'NNP']],\n",
       " [['deleted', 'VBN'], ['.', '.'], ['Jimbo', 'NNP']],\n",
       " [['made', 'VBN'], ['by', 'IN'], ['Jimbo', 'NNP']]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.array(jimbo).tolist() for jimbo in jimbos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would need to get rid of the punctuation characters for this to be truly effective!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...at this point the book goes into a long disccussion on the subject of corpus tagging. All very interesting, but not useful here...I did read it.\n",
    "\n",
    "Of particular interest is the **sparse data problem**. Simple **n-gram tagging** is done by assigning the likeliest tags to n-length combinations of words determined by training data (which is catalouged by hand). In case that an n-gram contains a new word that has never been seen before, however, an n-gram tagger will fail on *every* n-gram containing that word: two n-grams in the case of bigrams (`new word`, `old word`; `old word`, `new word`), three in the case of trigrams, and so on.\n",
    "\n",
    "This is very intrinsically related to the precision-recall tradeoff.\n",
    "\n",
    "Nevertheless, the accuracy of NLP classifiers using n-gram identification with fallbacks was an encouragement to research groups back when it was state-of-the-art (in the 90s), as it requires little lexical knowledge of the text to classify right much of the time.\n",
    "\n",
    "A **Brill tagger** is a guess-and-reguess tagger and a form of supervised learning which identifies thousands of rules, using random probing tag-and-compare against the gold version of the text, and combines the best of these to implement a \"tag-by-modification\" type of tagger.\n",
    "\n",
    "That was chapter 6. Chapter 7 has lots more text classification materials, including some information on hidden Markov models:\n",
    "\n",
    "> One shortcoming of this approach is that we commit to every decision that we make. For example, if we decide to label a word as a noun, but later find evidence that it should have been a verb, there's no way to go back and fix our mistake. One solution to this problem is to adopt a transformational strategy instead. Transformational joint classifiers work by creating an initial assignment of labels for the inputs, and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs. The Brill tagger, described in (1), is a good example of this strategy.\n",
    ">\n",
    "> Another solution is to assign scores to all of the possible sequences of part-of-speech tags, and to choose the sequence whose overall score is highest. This is the approach taken by Hidden Markov Models. Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags. However, rather than simply finding the single best tag for a given word, they generate a probability distribution over tags. These probabilities are then combined to calculate probability scores for tag sequences, and the tag sequence with the highest probability is chosen. Unfortunately, the number of possible tag sequences is quite large. Given a tag set with 30 tags, there are about 600 trillion (3010) ways to label a 10-word sentence. In order to avoid considering all these possible sequences separately, Hidden Markov Models require that the feature extractor only look at the most recent tag (or the most recent n tags, where n is fairly small). Given that restriction, it is possible to use dynamic programming (4.7) to efficiently find the most likely tag sequence. In particular, for each consecutive word index i, a score is computed for each possible current and previous tag. This same basic approach is taken by two more advanced models, called Maximum Entropy Markov Models and Linear-Chain Conditional Random Field Models; but different algorithms are used to find scores for tag sequences.\n",
    "\n",
    "RTE is fascinating! IBM Watson, as a quasi-institution, is built atop this basic building task.\n",
    "\n",
    "The rest of the chapter is tons of somewhat tedious explanations of machine learning tasks, with examples drawn from NLP problems.\n",
    "\n",
    "...well, it appears that NLTK doesn't come with a built-in chunker, so we'll have to build one ourselves!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:signpost-tags]",
   "language": "python",
   "name": "conda-env-signpost-tags-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
