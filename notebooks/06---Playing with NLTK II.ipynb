{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but doing part-of-speech tagging is both slow (it uses a neural network underneath!) and memory intensive. Very memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"../data/signpost_corpus.txt\", \"r\") as f:\n",
    "#     signpost_tagged_pos = nltk.pos_tag(nltk.word_tokenize(f.read())[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del signpost_tagged_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watching Task Manager says that this op took `0.28 GB` of memory, whilst processing just 1/445th of the total data. That means that processing the entire corpus (assuming linear performance) would take `~124 GB`, obvious absurd.\n",
    "\n",
    "Ok, so don't do it directly. Got it.\n",
    "\n",
    "Trying to read the tokens into a numpy array also didn't work, unfortunately, raising a memory error. A `np.array(tokens[:1000000])` object (representing <25% of the data) knocks out `~2 GB` of memory. I'm very clearly reaching the limits of my hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "with open(\"../data/signpost_corpus.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    sentences = nltk.sent_tokenize(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Association of Members' Advocates, an independent association involved in the dispute resolution process, has started discussions on having a new election for the position of AMA Coordinator.\",\n",
       " 'Since the initial election of Alex756 to this position last April was for a term of six months, it appears the new election is already several months overdue.',\n",
       " 'A poll on the question of having new elections was running strongly in favor, but general discussion among the members was a little more uncertain how to proceed.',\n",
       " 'Voicing an outside opinion on the AMA, Ambi said she thought it was \"completely dead\" and hadn\\'t seen it in action anywhere.',\n",
       " \"Some advocates responded that their work hadn't been done in the later stages of the dispute resolution system, noting that they seemed mostly to be helping newcomers who weren't familiar with Wikipedia procedures.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172472"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.44944"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".2*172472/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a hopeful estimate of how much memory would be required to store (based on some now-deleted tests). I only have `8 GB` of `RAM` available on my Desktop at the moment, I will buy more&mdash;I've clearly hit the limits of my hardware. We'll just take the most recent 100,000 sentences published in the *Signpost* instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentences_pos = np.concatenate([nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in sentences[-100000:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottleneck isn't the classifier itself, which barely uses any memory, it's writing into a `numpy` array afterwards and retaining that. The write process above took `~4 GB` to execute, spiking me to my RAM limit, before falling back down to \"just\" ~3.6 GB, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['The', 'DT'],\n",
       "       ['Malagasy', 'NNP'],\n",
       "       ['Wikipedia', 'NNP'],\n",
       "       ..., \n",
       "       ['his', 'PRP$'],\n",
       "       ['retirement', 'NN'],\n",
       "       ['.', '.']], \n",
       "      dtype='<U171')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3632656968"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pos.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dtype` hints that this huge memory drain is due to extremely long strings messing with the `numpy` stride formatting. I should have seen this coming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '//www.lsi.upc.edu/~tsteiner/papers/2013/mj-no-more-using-concurrent-wikipedia-edit-spikes-with-social-network-plausibility-checks-for-breaking-news-detection-ramss2013.pdf',\n",
       "       'JJ'], \n",
       "      dtype='<U171')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pos[np.argmax([len(word) for word, _ in sentences_pos])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well:\n",
    "\n",
    "> The longest word in the Oxford English Dictionary is ... 'Supercalifragilisticexpialidocious', made famous by Mary Poppins, [which] is 34 letters long.\n",
    "\n",
    "So let's cast to `dtype=\"<U34\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['The', 'DT'],\n",
       "       ['Malagasy', 'NNP'],\n",
       "       ['Wikipedia', 'NNP'],\n",
       "       ..., \n",
       "       ['his', 'PRP$'],\n",
       "       ['retirement', 'NN'],\n",
       "       ['.', '.']], \n",
       "      dtype='<U34')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(map(lambda arr: arr.astype(\"|U34\") , sentences_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. Let's use this as a workaround from the start."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:signpost-tags]",
   "language": "python",
   "name": "conda-env-signpost-tags-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
